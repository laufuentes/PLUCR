---
title: "Introduction to Policy Learning Under Constraint R-package (PLUCR)"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{PLUCR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = TRUE,        # show code
  message = TRUE,     # show messages from code
  warning = TRUE      # show warnings from code
)
```

# 1. Introduction

The `PLUCR` package provides tools for estimating treatment policies that maximize a primary clinical outcome while controlling for the occurrence of adverse events. This dual-objective framework is particularly relevant in medical or decision-making settings where benefits must be balanced against potential harms.

Many existing methods derive treatment policies based on the sign of the Conditional Average Treatment Effect (CATE). In this setup, individuals with a positive CATE are assigned treatment, as it is expected to be beneficial; those with a non-positive CATE are not treated. While effective in simple settings, this strategy becomes inadequate when multiple outcomes must be considered â€” particularly when adverse events are ignored.

`PLUCR` addresses this limitation by framing the policy learning task as a constrained optimization problem. The goal is to learn a policy that not only improves the primary outcome but also respects pre-specified constraints on adverse effects. The method implemented in `PLUCR` estimates treatment policies by adapting the Conditional Average Treatment Effect (CATE) to ensure compliance with a predefined constraint on adverse events. This is achieved through an iterative procedure that alternates between optimizing the policy and correcting its estimator, ultimately leading to a constrained-optimal policy.

# 2. Getting started 

This section demonstrates a basic example of how to use the `PLUCR` package. We begin by generating a synthetic dataset and applying the main function, `main_algorithm()`, to learn an optimal policy under adverse event constraints.

## 2.1. Installation

You can install the development version from your local source:

```{r,eval=FALSE}
devtools::install_github("laufuentes/PLUCR")
```

## 2.2. Load required packages and data
```{r, eval=FALSE}
library(PLUCR)
library(SuperLearner)
library(grf)
library(dplyr)
library(tidyr)
library(ggplot2)
```

We generate synthetic observational data using the built-in `generate_data()` function. This function also produces the complete data structure (which is typically unavailable in real-world applications and used here for evaluation purposes only) as well as the oracular treatment effect functions `delta_mu()` and `delta_nu()`.

```{r, eval=FALSE}
n <- 3000  # Sample size
ncov <- 10L # Number of covariates
scenario_mu <- "Threshold"
scenario_nu <- "Threshold"

# Generate synthetic scenario
exp <- PLUCR::generate_data(n, 
                            ncov, 
                            scenario_mu = scenario_mu, 
                            scenario_nu = scenario_nu, 
                            seed=2025) 

# # Complete data (not available in real settings)
df_complete <- exp[[1]] 

# # Observational data (to be used for training)
df_obs <- exp[[2]]

# # Oracular delta mu function (treatment effect over Y conditionned on X) 
delta_mu <- exp[[3]]

# # Oracular delta mu function (treatment effect over Y conditionned on X) 
delta_nu <- exp[[4]]
```

## 2.3. Prepare Variables and Function parameters

In this step, we extract the necessary variables from the observational dataset and define the parameters required by the `main_algorithm()` function. This includes specifying the covariates, treatment indicator, outcomes, and adverse events.

```{r,eval=FALSE}
# Covariates matrix 
X <- df_obs %>%
  dplyr::select(starts_with("X."))%>%
  as.matrix()

# Binary treatment assignment (0 = control, 1 = treated)
A <- df_obs$Treatment

# Primary outcome, assumed to be scaled to [0, 1]
Y <- df_obs$Y

# Binary indicator of adverse events (0=No adverse event, 1=adverse event)
Xi <- df_obs$Xi
```

We now define the parameters to be passed to `main_algorithm()`. These include the learners used in `SuperLearner` estimation, the directory for saving results, and tuning parameters that control constraint enforcement and optimization precision.

```{r, eval=FALSE}
# SuperLearner library used for estimating nuisance functions
# You can customize this list. The default includes:
# - SL.mean: simple mean predictor
# - SL.glm: generalized linear model
# - SL.ranger: random forest
# - SL.grf: generalized random forest
# - SL.xgboost: gradient boosting
SL.library <- c("SL.mean", "SL.glm", "SL.ranger", "SL.grf", "SL.xgboost")

# Insert a root directory where output files, results and images will be saved
root.path <- "~/results"

# An increasing sequence of non-negative numeric scalars controlling 
# the penalty for violating the constraint (seq(1,8, by=1) by default).
Lambda_candidates <- seq(1, 10, by=1)

# A vector of non-negative scalars controlling the sharpness of the 
# treatment probability function (c(0.05, 0.1, 0.25, 0.5) by default)
Beta_candidates <- c(0.05, 0.1, 0.25, 0.5)

# Constraint tolerance (alpha): maximum allowed increase in adverse event risk
# Example: 0.15 means the policy may increase adverse event probability by up to 15%
alpha.tolerance <- 0.2

# Optimization precision: controls granularity of the policy search
# Smaller values lead to finer approximations but require more computation
optimization.precision <- 0.05

# Tolerance parameter for Optimization-Estimation procedure
# Smaller values lead to finer approximations but require more computation
Opt.Est.tolerance <- 0.025

# Optimization Estimation procedure maximum of iterations
Opt.Est.Maximum.Iterations <- 5
```

## 2.4. Run the main algorithm 
With all variables and parameters defined, we now apply the main function `main_algorithm()` from the `PLUCR` package. This function performs the entire estimation pipeline, including:

1- Estimating nuisance components using `SuperLearner`,

2- Learning a treatment policy that maximizes the primary outcome, enforcing a constraint on the probability of adverse events with the alternated procedure. 

The output includes both:

- An unconstrained policy (based solely on primary outcome maximization)

- A constrained-optimal policy (that respects the adverse event constraint).
```{r, eval=FALSE}
output <- PLUCR::main_algorithm(X=X, A=A, Y=Y, Xi=Xi, 
                                Lambdas = Lambda_candidates,
                                alpha=alpha.tolerance, 
                                B= Beta_candidates, 
                                precision=optimization.precision,
                                SL.library = SL.library, 
                                tol= Opt.Est.tolerance, 
                                max_iter=Opt.Est.Maximum.Iterations, 
                                root.path=root.path)

if(is.list(output)){
  theta_0 <- output[[1]]
  theta_final <- output[[2]]
}else{
  theta_final <- output
}
```

```{r, eval=FALSE}
folds <- readRDS(file.path(root.path,"Folds","folds.rds"))
mu.hat.nj <- readRDS(file.path(root.path,"Mu.hat","mu.hat.nj.rds")) #Save primary outcome model
# Adverse event model
nu.hat.nj <- readRDS(file.path(root.path,"Nu.hat","nu.hat.nj.rds")) #Save adverse events outcome model
# Propensity score
ps.hat.nj <- readRDS(file.path(root.path,"PS.hat","ps.hat.nj.rds")) #Save propensity score outcome model
  
### Training data & functions
# data
X_train <- X[folds[[2]],]
A_train <- A[folds[[2]]]
Y_train <- Y[folds[[2]]]
Xi_train <- Xi[folds[[2]]]
# functions
mu0_train <- function(a,x){mu.hat.nj(a=a,x=x,ff=1)}
nu0_train <- function(a,x){nu.hat.nj(a=a,x=x,ff=1)}
prop_score_train <- function(a,x){ps.hat.nj(a,x,1)}
  
### Testing data & functions
# data
X_test <- X[folds[[3]],]
A_test <- A[folds[[3]]]
Y_test <- Y[folds[[3]]]
Xi_test <- Xi[folds[[3]]]
# functions
mu0_test <- function(a,x){mu.hat.nj(a,x,3)}
nu0_test <- function(a,x){nu.hat.nj(a,x,3)}
prop_score_test <- function(a,x){ps.hat.nj(a,x,3)}

naive_output <- naive_approach_algorithm(X, A, Y, Xi, folds, mu0_train, mu0_test, nu0_train, nu0_test, 
                                         prop_score_train, prop_score_test, 
                                         alpha = alpha.tolerance, precision=optimization.precision,
                                         root.path = root.path)

if(is.list(naive_output)){
 theta_naive <- naive_output[[2]] 
}else{
  theta_naive <- naive_output[[1]]
}
eval_naive <- process_results(theta=theta_naive, X=X_test, A=A_test, Y=Y_test, Xi=Xi_test, 
                              mu0=mu0_test, nu0=nu0_test, prop_score=prop_score_test, 
                              lambda = attr(theta_naive, "lambda"), alpha = alpha.tolerance,
                              beta= attr(theta_naive, "beta"))[[1]]
```


```{r, eval=FALSE}
oracular_output <- oracular_approach_algorithm(X=X, A=A, Y=Y, Xi=Xi, 
                                               folds=folds,
                                               delta_Mu=delta_mu, delta_Nu= delta_nu, 
                                               alpha = alpha.tolerance, precision = optimization.precision, 
                                               scenario_mu=scenario_mu, scenario_nu=scenario_nu, 
                                               root.path = root.path)

if(is.list(oracular_output)){
 theta_oracular <- oracular_output[[2]] 
}else{
  theta_oracular <- oracular_output[[1]]
}
eval_oracular <- oracular_process_results(theta=theta_oracular, X=X_test,  
                                          delta_Mu = delta_mu, delta_Nu = delta_nu, 
                                          scenario_mu = scenario_mu, scenario_nu = scenario_nu, 
                                          lambda = attr(theta_oracular, "lambda"), 
                                          alpha = alpha.tolerance, beta= attr(theta_oracular, "beta"))
```

# 3. Play with results 
The function `synthetic_data_plot()` provides a visualization of the treatment effect for primary outcomes and adverse events. 
```{r, eval=FALSE}
fig <- PLUCR::synthetic_data_plot(delta_Mu=delta_mu, delta_Nu=delta_nu, root.path=root.path, name="Threshold-Threshold")

print(fig)
```

However, we can visualize both treatment rules using variables that might ensure a proper visualization. 
```{r, eval=FALSE}
PLUCR::visual_treatment_plot(make_psi(theta_0)(X), 
                              lambda = attr(theta_0, "lambda"), 
                              beta = attr(theta_0, "beta"), 
                              centered = FALSE, Var_X_axis = df_obs$X.3, Var_Y_axis = df_obs$X.4,
                              root.path = root.path, name = "Initial")

 PLUCR::visual_treatment_plot(make_psi(theta_naive)(X), 
                              lambda = attr(theta_naive, "lambda"), 
                              beta = attr(theta_naive, "beta"), 
                              centered = FALSE, Var_X_axis = df_obs$X.3, Var_Y_axis = df_obs$X.4,
                              root.path = root.path, name = "Naive")
 
 PLUCR::visual_treatment_plot(make_psi(theta_final)(X), 
                              lambda = attr(theta_final, "lambda"), 
                              beta = attr(theta_final, "beta"), 
                              centered = FALSE, Var_X_axis = df_obs$X.3, Var_Y_axis = df_obs$X.4,
                              root.path = root.path, name = "Final")
 
 PLUCR::visual_treatment_plot(make_psi(theta_oracular)(X), 
                              lambda = attr(theta_oracular, "lambda"), 
                              beta = attr(theta_oracular, "beta"), 
                              centered = FALSE, Var_X_axis = df_obs$X.3, Var_Y_axis = df_obs$X.4,
                              root.path = root.path, name = "Oracular")
```


```{r, eval=FALSE}
eval_0 <- readRDS(file.path(root.path, "Evaluation", paste0(attr(theta_0, "beta"), "_", attr(theta_0, "lambda"), ".rds")))

eval_final <- readRDS(file.path(root.path, "Evaluation", paste0(attr(theta_final, "beta"), "_", attr(theta_final, "lambda"), ".rds")))


data <- tibble(
  method = c("Theta_0", "Theta_naive", "Theta_final", "Theta_oracular"),
  policy_value = c(eval_0$policy_value, eval_naive$policy_value, eval_final$policy_value, eval_oracular$policy_value),
  constraint = c(eval_0$constraint, eval_naive$constraint, eval_final$constraint, eval_oracular$constraint)
)

# Call the function
plot_metric_comparison(data, metrics = select(data, -"method") %>% colnames(), techniques = data$method, root.path = root.path)
```


# Appendix. Understanding the Algorithm Internally

The `main_algorithm()` function is designed to abstract away the technical details while still allowing flexibility through its parameters. Internally, it carries out a constrained policy learning procedure, broken down into three main stages, each performed on a different fold. 

## Preliminary Step: Data Checks and Cross-Fitting Folds
Before estimating nuisance functions and optimizing the policy, the algorithm performs three important preliminary tasks:

- Creates the folds where results will be saved. 

- Verifies that the input variables (X, A, Y and Xi) meet the required format and assumptions, such as outcome ranges, binary treatment/adverse event values.

- Creates 3 cross-validation folds to enable cross-fitting. 
```{r, eval=FALSE}
n <- nrow(X)

# Folds for cross-validation 
folds <- SuperLearner::CVFolds(n, 
                               id = NULL,
                               Y = Y,
                               cvControl = SuperLearner::SuperLearner.CV.control(V = JFold, 
                                                                                 shuffle =TRUE))
# Check data
checks<- PLUCR::check_data(Y, Xi, A, X, folds)
```

## Step 1: Estimate and Save Nuisance Parameters

Once the variables have been validated and the cross-fitting folds are defined, the algorithm proceeds to estimate the nuisance parameters. These models are each trained using the pre-defined folds to enable cross-fitting, which improves robustness and reduces overfitting.

The key nuisance parameters estimated are:

- **Primary outcome model:** $E[Y \mid A, X]$  
- **Adverse event model:** $E[\xi \mid A, X]$  
- **Propensity score model:** $P[A \mid X]$  

Each model is fit separately and the estimates are saved for later use in policy learning.

```{r, eval=FALSE}
# Estimate primary outcome model E[Y | A, X]
mu.hat.nj <- PLUCR::estimate_mu(
  Y = Y, A = A, X = X, folds = folds,
  SL.library = SL.library, V = 2L
)
saveRDS(mu.hat.nj, file = file.path(root.path, "Mu.hat", "mu.hat.nj.rds"))

# Estimate adverse event model E[Xi | A, X]
nu.hat.nj <- PLUCR::estimate_nu(
  Xi = Xi, A = A, X = X, folds = folds,
  SL.library = SL.library, V = 2L
)
saveRDS(nu.hat.nj, file = file.path(root.path, "Nu.hat", "nu.hat.nj.rds"))

# Estimate propensity score model P[A | X]
ps.hat.nj <- PLUCR::estimate_ps(
  A = A, X = X, folds = folds,
  SL.library = SL.library, V = 2L
)
saveRDS(ps.hat.nj, file = file.path(root.path, "PS.hat", "ps.hat.nj.rds"))
```

After estimating the nuisance parameters, the next step is to organize the data into **training** and **testing** sets. This allows us to:

- Learn nuisance components on a first training subset of the data. 
- Estimate the optimal treatment policy on a second independent training set of the data.
- Evaluate the learned policyâ€™s performance on an independent test set.

```{r, eval=FALSE}
# Training functions & data (first and second folds)
# functions (first fold)
mu0_train <- function(a,x){mu.hat.nj(a=a,x=x,ff=1)}
nu0_train <- function(a,x){nu.hat.nj(a=a,x=x,ff=1)}
prop_score_train <- function(a,x){ps.hat.nj(a,x,1)}

# data (second fold)
X_train <- X[folds[[2]],]
A_train <- A[folds[[2]]]
Y_train <- Y[folds[[2]]]
Xi_train <- Xi[folds[[2]]]
  

# Testing functions & data (third fold)
# functions
mu0_test <- function(a,x){mu.hat.nj(a,x,3)}
nu0_test <- function(a,x){nu.hat.nj(a,x,3)}
prop_score_test <- function(a,x){ps.hat.nj(a,x,3)}

# data (third fold)
X_test <- X[folds[[3]],]
A_test <- A[folds[[3]]]
Y_test <- Y[folds[[3]]]
Xi_test <- Xi[folds[[3]]]
```

## Step 2: Policy optimization

In this step, we run the core algorithm of the `PLUCR` package â€” the **alternating optimization procedure** implemented in the `Optimization_Estimation()` function. This procedure learns an optimal treatment policy that maximizes the expected primary outcome while controlling the probability of adverse events.

The procedure is executed over a grid of $(\lambda, \beta)$ values, where:

- $\lambda$ controls the trade-off between maximizing the primary outcome and satisfying the constraint.
- $\beta$ is a tuning parameter influencing the optimization regularization.

Before evaluating the entire grid, the algorithm first solves the problem with $\lambda = 0$ in order to check whether the unconstrained optimal policy already satisfies the constraint. If it does, no further optimization over the grid is necessary.


During the search over $\lambda$, once the constraint is satisfied for the first time, the procedure saves the corresponding results at each $\beta$ value. The search over $\lambda$ then stops when the upper bound of the confidence interval for the constraint value drops below zero.

**Alternating Optimization Procedure**

Whether with $\lambda = 0$ or $\lambda > 0$, the procedure iterates as follows:

1. Initial estimation of the plug-in objective function $L(\psi)$ using $\Delta\mu_n^k$ and $\Delta\nu_n^k$ derived from, 
   - $\mu_n^k$: the expected outcome model (for k=0, this corresponds to `mu0_train`).
   - $\nu_n^k$: the adverse event model (for k=0, this corresponds to `nu0_train`).

2. **Policy optimization**: Frank- Wolfe algorithm 
   Minimize $L^k_n(\psi)$ to obtain a new candidate policy $\psi_k$.

3. **Update nuisance parameters**:
   - Estimate $\epsilon_1$ and $\epsilon_2$ to correct the outcome and adverse event models using all previous solutions $(\psi_0, ..., \psi_k)$, yielding updated models  $\mu_n^{k+1}$, $\nu_n^{k+1}$ (using functions `update_mu_XA`, `update_nu_XA`).

4. Repeat until convergence or until the maximum number of iterations is reached.
```{r, eval=FALSE}
# Start by testing the unconstrained case with lambda = 0
saved <- FALSE  # Flag to track whether a valid result has already been saved
combinations <- NULL  # Matrix to store (beta, lambda) pairs that satisfy the constraint
beta <- 0.05  # Arbitrary beta value since constraint is not evaluated yet
lambda <- 0

# Run optimization with lambda = 0 (no constraint)
out <- PLUCR::Optimization_Estimation(mu0_train, nu0_train, prop_score_train, 
                                        X_train, A_train, Y_train, Xi_train, 
                                        lambda=0, alpha, precision, beta, centered, 
                                        file.path(root.path,"Intermediate",paste0(0,"_",beta)))

# Extract the final policy from the iteration  
theta_0 <- out$theta_collection[[length(out$theta_collection)]]
# Evaluate the policy on the test data
res_0 <- process_results(theta_0, 
                         X_test, A_test, Y_test, Xi_test, 
                         mu0_test, nu0_test, prop_score_test, 
                         lambda=0, alpha,  beta, centered)
# Save the evaluation results
saveRDS(res_0[[1]], file=file.path(root.path,"Evaluation",paste0(beta,"_",0,".rds")))
  
# If the constraint is satisfied, save the corresponding policy and mark as saved
if (!saved && res_0[[1]]$constraint < 0) {
    saveRDS(theta_0, file = file.path(root.path, "Theta_opt", paste0(beta, "_", 0, ".rds")))
    psi_0 <- make_psi(theta_opt)
    sigma_beta_0 <- sigma_beta(psi_0(X), beta=0.05, centered = FALSE)
    combinations <- rbind(combinations, c(beta, lambda))
    saved <- TRUE
}
# If the upper bound of the constraint's CI is below zero, stop here
if(res_0[[2]]<0){
    print(theta_0)
}else{
    # Begin training over the grid: find for each beta value the optimal lambda
    for (beta in Betas){
      saved <- FALSE
      for (lambda in Lambdas){
       # Run alternated procedure for each beta and lambda combination 
        out <- PLUCR::Optimization_Estimation(mu0_train, nu0_train, prop_score_train, 
                                              X_train, A_train, Y_train, Xi_train, 
                                              lambda, alpha, precision, beta, centered,
                                              file.path(root.path,"Intermediate", 
                                                        paste0(lambda,"_",beta)))
        # Extract final policy
        theta_opt <- out$theta_collection[[length(out$theta_collection)]]
         # Evaluate the policy on test set
        res <- process_results(theta_opt, 
                               X_test, A_test, Y_test, Xi_test, 
                               mu0_test, nu0_test, prop_score_test, 
                               lambda, alpha, beta, centered)
        # Save evaluation results
        saveRDS(res[[1]], file=file.path(root.path, "Evaluation", 
                                         paste0(lambda,"_",beta,".rds")))
        
        # If constraint is satisfied for the first time, save the optimal policy
        if (!saved && res[[1]]$constraint < 0) {
          saveRDS(theta_opt, 
                  file = file.path(root.path, "Theta_opt", paste0(lambda, "_", beta, ".rds")))
          saved <- TRUE
          psi_opt <- make_psi(theta_opt)
          sigma_beta_opt <- sigma_beta(psi_opt(X), beta=0.05, centered = FALSE)
          combinations <- rbind(combinations, c(beta, lambda))
        }
        # Stop search if constraint's upper CI bound is negative
        if(res[[2]]<0){
          break
        }
      }
    }
  # Load all lambda-optimal results and select the (beta, lambda) combination 
  # that maximizes the policy value
  optimal_combination <- get_opt_beta_lambda(combinations,root.path)
  
  # Load the corresponding theta for the optimal (beta, lambda) combination
  theta_final <- readRDS(file = file.path(root.path, 
                                          "Theta_opt", 
                                          paste0(optimal_combination$beta, "_", 
                                                 optimal_combination$lambda, ".rds"))) 
  }
print(list(theta_0, theta_final))
```

